## sklearn 聚类算法 实验报告

### 运行代码

```bash
python src/main.py -f data/Tweets.txt -l log/logfile.log
```

### 实验环境与实验数据

* CPU -- i7-8700K
* 内存 -- 15.6 GB
* 实验数据: [Tweets.txt](data/Tweets.txt)

### 实验目的

熟悉 sklearn 库中的常用聚类算法 API, 对 Tweets 数据进行聚类, 并比较效果.

### 实验步骤

#### 读取数据

对 『Tweets.txt 』中的数据进行读取, 每一条数据都是是 『JSON 』格式的,所以通过 Python 的 『json』 模块对 『Tweets.txt』 中数据的每一行进行读取.

实现文件: 『src/main.py』

#### 聚类及评估

通过 sklearn 库中的 API, 对数据进行聚类.运用了如下几个方法:

* KMeans
* AffinityPropagation
* MeanShift
* SpectralClustering
* Ward Hierarchical Clustering
* AgglomerativeClustering
* DBSCAN
* GaussianMixture

进行聚类.通过 NMI 对聚类结果进行评价.

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20150317154915532)



其中I(A,B)是A,B两向量的mutual information, H(A)是A向量的信息熵。 

##### 1. KMeans

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-KrcZK0xYgTa4qFrVr0fO2w.gif)

1. 首先，选择一些类/组来使用并随机地初始化它们各自的中心点。
2. 每个数据点通过计算点和每个组中心之间的距离进行分类，然后将这个点分类为最接近它的组。
3. 基于这些分类点，我们通过取组中所有向量的均值来重新计算组中心。
4. 对一组迭代重复这些步骤。

结果: KMeans cost time 38.968318462371826s, score is 0.781562372454415 

##### 2. AffinityPropagation

![img](http://scikit-learn.org/stable/_images/sphx_glr_plot_affinity_propagation_001.png)

AP算法的基本思想是将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。A

P算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。

结果: AffinityPropagation cost time 13.451494693756104s, score is 0.7836988975391974 

##### 3. MeanShift

均值偏移（Mean shift）聚类算法是一种基于滑动窗口（sliding-window）的算法，它试图找到密集的数据点。而且，它还是一种基于中心的算法，它的目标是定位每一组群/类的中心点，通过更新中心点的候选点来实现滑动窗口中的点的平均值。这些候选窗口在后期处理阶段被过滤，以消除几乎重复的部分，形成最后一组中心点及其对应的组。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-vyz94J_76dsVToaa4VG1Zg.gif)

与K-Means聚类相比，均值偏移不需要选择聚类的数量，因为它会自动地发现这一点。这是一个巨大的优势。聚类中心收敛于最大密度点的事实也是非常可取的，因为它非常直观地理解并适合于一种自然数据驱动。

缺点是选择窗口大小/半径r是非常关键的，所以不能疏忽。

结果: MeanShift cost time 19.27985906600952s, score is 0.7278545708737196

##### 4. SpectralClustering

![img](https://img-blog.csdn.net/20161102063832348)

![img](https://img-blog.csdn.net/20161102074558409)

1. 根据输入的相似矩阵的生成方式构建样本的相似矩阵S
2. 根据相似矩阵S构建邻接矩阵W，构建度矩阵D
3. 计算出拉普拉斯矩阵L
4. 构建标准化后的拉普拉斯矩阵
5. 计算拉普拉斯矩阵最小的$k_1$ 个特征值所各自对应的特征向量
6. 将各自对应的特征向量ff组成的矩阵按行标准化，最终组成$n * k_1$维的特征矩阵F
7. 对F中的每一行作为一个$k_1$维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为$k_2$。
8. 得到簇划分C(c1,c2,...ck2).　

结果: SpectralClustering cost time 40.73340439796448s, score is 0.8067027949127966 

##### 5. AgglomerativeClustering

层次聚类（hierarchical clustering）可在不同层次上对数据集进行划分，形成树状的聚类结构。AggregativeClustering是一种常用的层次聚类算法。

其原理是：最初将每个对象看成一个簇，然后将这些簇根据某种规则被一步步合并，就这样不断合并直到达到预设的簇类个数。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-ET8kCcPpr893vNZFs8j4xg.gif)

1. 我们首先将每个数据点作为一个单独的聚类进行处理。
2. 在每次迭代中，我们将两个聚类合并为一个。
3. 重复步骤2直到我们到达树的根。层次聚类算法不要求我们指定聚类的数量，我们甚至可以选择哪个聚类看起来最好。此外，该算法对距离度量的选择不敏感.

结果: AgglomerativeClustering cost time 7.590268611907959s, score is 0.7847178748775534 

##### 6. DBSCAN

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-tc8UF-h0nQqUfLC8-0uInQ.gif)

DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于密度的聚类算法，类似于均值转移聚类算法.

1. DBSCAN以一个从未访问过的任意起始数据点开始。
2. 如果在这个邻域中有足够数量的点（根据 minPoints），那么聚类过程就开始了，并且当前的数据点成为新聚类中的第一个点。否则，该点将被标记为噪声（稍后这个噪声点可能会成为聚类的一部分）。在这两种情况下，这一点都被标记为“访问（visited）”。
3. 对于新聚类中的第一个点，其ε距离附近的点也会成为同一聚类的一部分。
4. 步骤2和步骤3的过程将重复，直到聚类中的所有点都被确定，就是说在聚类附近的所有点都已被访问和标记。
5. 一旦我们完成了当前的聚类，就会检索并处理一个新的未访问点，这将导致进一步的聚类或噪声的发现。这个过程不断地重复，直到所有的点被标记为访问。

结果: DBSCAN cost time 46.81977677345276s, score is 0.611995415736244 

##### 7. GaussianMixture

K-Means的一个主要缺点是它对聚类中心的平均值的使用很简单。

高斯混合模型（GMMs）比K-Means更具灵活性。使用高斯混合模型，我们可以假设数据点是高斯分布的;比起说它们是循环的，这是一个不那么严格的假设。这样，我们就有两个参数来描述聚类的形状：平均值和标准差以二维的例子为例，这意味着聚类可以采用任何形式的椭圆形状（因为在x和y方向上都有标准差）。因此，每个高斯分布可归属于一个单独的聚类。

为了找到每个聚类的高斯分布的参数（例如平均值和标准差）我们将使用一种叫做期望最大化（EM）的优化算法。看看下面的图表，就可以看到高斯混合模型是被拟合到聚类上的。然后，我们可以继续进行期望的过程——使用高斯混合模型实现最大化聚类。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-OyXgise21a23D5JCss8Tlg.gif)

1. 我们首先选择聚类的数量（如K-Means所做的那样），然后随机初始化每个聚类的高斯分布参数。
2. 给定每个聚类的高斯分布，计算每个数据点属于特定聚类的概率。
3. 基于这些概率，我们为高斯分布计算一组新的参数，这样我们就能最大程度地利用聚类中的数据点的概率。
4. 步骤2和3被迭代地重复，直到收敛。

结果: GaussianMixture cost time 5.616118431091309s, score is 0.7834852894647376

### 实验结果

需要对聚类方法的效果以及时间 cost 进行综合评估, 发现虽然 SpectralClustering 的方法准确率是最高的,但是耗费时间也是最长的.综合看下来 GaussianMixture 的效果最好.
