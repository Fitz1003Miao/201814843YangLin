## 第三次实验报告

### 运行代码

```bash
python src/main.py -f data/Tweets.txt -l log/logfile.log
```

### 实验环境与实验数据

* CPU -- i7-8700K
* 内存 -- 15.6 GB
* 实验数据: [Tweets.txt](data/Tweets.txt)

### 实验目的

熟悉 sklearn 库中的常用聚类算法 API, 对 Tweets 数据进行聚类, 并比较效果.

### 实验步骤

#### 读取数据

对 『Tweets.txt 』中的数据进行读取, 每一条数据都是是 『JSON 』格式的,所以通过 Python 的 『json』 模块对 『Tweets.txt』 中数据的每一行进行读取.

实现文件: 『src/main.py』

#### 聚类

通过 sklearn 库中的 API, 对数据进行聚类.运用了如下几个方法:

* KMeans
* AffinityPropagation
* MeanShift
* SpectralClustering
* Ward Hierarchical Clustering
* AgglomerativeClustering
* DBSCAN
* GaussianMixture

进行聚类.

##### 1. KMeans

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-KrcZK0xYgTa4qFrVr0fO2w.gif)

1. 首先，选择一些类/组来使用并随机地初始化它们各自的中心点。

2. 每个数据点通过计算点和每个组中心之间的距离进行分类，然后将这个点分类为最接近它的组。

3. 基于这些分类点，我们通过取组中所有向量的均值来重新计算组中心。

4. 对一组迭代重复这些步骤。

K-Means聚类算法的优势在于它的速度非常快，因为所做的只是计算点和群中心之间的距离;它有一个线性复杂度*O*(*n*)。

另一方面，K-Means也有几个缺点。首先，你必须选择有多少组/类。这并不是不重要的事，理想情况下，我们希望它能帮我们解决这些问题，因为它的关键在于从数据中获得一些启示。K-Means也从随机选择的聚类中心开始，因此在不同的算法运行中可能产生不同的聚类结果。因此，结果可能是不可重复的，并且缺乏一致性。其他聚类方法更加一致。

对应 sklearn 中的 API 为: KMeans

##### 2. AffinityPropagation

![img](http://scikit-learn.org/stable/_images/sphx_glr_plot_affinity_propagation_001.png)

AP算法的基本思想是将全部样本看作网络的节点，然后通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点间传递，分别是吸引度( responsibility)和归属度(availability) 。AP算法通过迭代过程不断更新每一个点的吸引度和归属度值，直到产生m个高质量的Exemplar（类似于质心），同时将其余的数据点分配到相应的聚类中。

AP聚类算法与 K-Means 类算法相比，具有很多独特之处：

1. 无需指定聚类“数量”参数。
2. 明确的质心（聚类中心点）。
3. 对距离矩阵的对称性没要求。
4. 初始值不敏感。
5. 算法复杂度较高，为O(N*N*logN)，而K-Means只是O(N*K)的复杂度。
6. 若以误差平方和来衡量算法间的优劣，AP聚类比其他方法的误差平方和都要低。

但是:

- AP聚类应用中需要手动指定Preference和Damping factor，这其实是原有的聚类“数量”控制的变体。
- 算法较慢。由于AP算法复杂度较高，运行时间相对K-Means长，这会使得尤其在海量数据下运行时耗费的时间很多。

##### 3. MeanShift

均值偏移（Mean shift）聚类算法是一种基于滑动窗口（sliding-window）的算法，它试图找到密集的数据点。而且，它还是一种基于中心的算法，它的目标是定位每一组群/类的中心点，通过更新中心点的候选点来实现滑动窗口中的点的平均值。这些候选窗口在后期处理阶段被过滤，以消除几乎重复的部分，形成最后一组中心点及其对应的组。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-vyz94J_76dsVToaa4VG1Zg.gif)

与K-Means聚类相比，均值偏移不需要选择聚类的数量，因为它会自动地发现这一点。这是一个巨大的优势。聚类中心收敛于最大密度点的事实也是非常可取的，因为它非常直观地理解并适合于一种自然数据驱动。

缺点是选择窗口大小/半径r是非常关键的，所以不能疏忽。

##### 4. SpectralClustering

![img](https://img-blog.csdn.net/20161102063832348)

![img](https://img-blog.csdn.net/20161102074558409)

1. 根据输入的相似矩阵的生成方式构建样本的相似矩阵S
2. 根据相似矩阵S构建邻接矩阵W，构建度矩阵D
3. 计算出拉普拉斯矩阵L
4. 构建标准化后的拉普拉斯矩阵
5. 计算拉普拉斯矩阵最小的$k_1$ 个特征值所各自对应的特征向量
6. 将各自对应的特征向量ff组成的矩阵按行标准化，最终组成$n * k_1$维的特征矩阵F
7. 对F中的每一行作为一个$k_1$维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为$k_2$。
8. 得到簇划分C(c1,c2,...ck2).　　

谱聚类算法的主要优点有：

1. 谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。

2. 由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。

谱聚类算法的主要缺点有：

1. 如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。

2. 聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。

##### 5. AgglomerativeClustering

层次聚类（hierarchical clustering）可在不同层次上对数据集进行划分，形成树状的聚类结构。AggregativeClustering是一种常用的层次聚类算法。

其原理是：最初将每个对象看成一个簇，然后将这些簇根据某种规则被一步步合并，就这样不断合并直到达到预设的簇类个数。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-ET8kCcPpr893vNZFs8j4xg.gif)

1. 我们首先将每个数据点作为一个单独的聚类进行处理。

2. 在每次迭代中，我们将两个聚类合并为一个。

3. 重复步骤2直到我们到达树的根。层次聚类算法不要求我们指定聚类的数量，我们甚至可以选择哪个聚类看起来最好。此外，该算法对距离度量的选择不敏感.

##### 6. DBSCAN

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-tc8UF-h0nQqUfLC8-0uInQ.gif)

DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于密度的聚类算法，类似于均值转移聚类算法.

1. DBSCAN以一个从未访问过的任意起始数据点开始。

2. 如果在这个邻域中有足够数量的点（根据 minPoints），那么聚类过程就开始了，并且当前的数据点成为新聚类中的第一个点。否则，该点将被标记为噪声（稍后这个噪声点可能会成为聚类的一部分）。在这两种情况下，这一点都被标记为“访问（visited）”。

3. 对于新聚类中的第一个点，其ε距离附近的点也会成为同一聚类的一部分。

4. 步骤2和步骤3的过程将重复，直到聚类中的所有点都被确定，就是说在聚类附近的所有点都已被访问和标记。

5. 一旦我们完成了当前的聚类，就会检索并处理一个新的未访问点，这将导致进一步的聚类或噪声的发现。这个过程不断地重复，直到所有的点被标记为访问。因为在所有的点都被访问过之后，每一个点都被标记为属于一个聚类或者是噪音。

DBSCAN比其他聚类算法有一些优势。首先，它不需要一个预设定的聚类数量。它还将异常值识别为噪声，而不像均值偏移聚类算法，即使数据点非常不同，它也会将它们放入一个聚类中。此外，它还能很好地找到任意大小和任意形状的聚类。

DBSCAN的主要缺点是，当聚类具有不同的密度时，它的性能不像其他聚类算法那样好。这是因为当密度变化时，距离阈值ε和识别邻近点的minPoints的设置会随着聚类的不同而变化。这种缺点也会出现在非常高维的数据中，因为距离阈值ε变得难以估计。

##### 7. GaussianMixture

K-Means的一个主要缺点是它对聚类中心的平均值的使用很简单。

高斯混合模型（GMMs）比K-Means更具灵活性。使用高斯混合模型，我们可以假设数据点是高斯分布的;比起说它们是循环的，这是一个不那么严格的假设。这样，我们就有两个参数来描述聚类的形状：平均值和标准差以二维的例子为例，这意味着聚类可以采用任何形式的椭圆形状（因为在x和y方向上都有标准差）。因此，每个高斯分布可归属于一个单独的聚类。

为了找到每个聚类的高斯分布的参数（例如平均值和标准差）我们将使用一种叫做期望最大化（EM）的优化算法。看看下面的图表，就可以看到高斯混合模型是被拟合到聚类上的。然后，我们可以继续进行期望的过程——使用高斯混合模型实现最大化聚类。

![5ç§ä¸"è¦èç±"ç®æ³çç®åä"ç"](http://imgcdn.atyun.com/2018/03/1-OyXgise21a23D5JCss8Tlg.gif)

1. 我们首先选择聚类的数量（如K-Means所做的那样），然后随机初始化每个聚类的高斯分布参数。

2. 给定每个聚类的高斯分布，计算每个数据点属于特定聚类的概率。

3. 基于这些概率，我们为高斯分布计算一组新的参数，这样我们就能最大程度地利用聚类中的数据点的概率。
4. 步骤2和3被迭代地重复，直到收敛。

使用高斯混合模型有两个关键的优势。

首先，高斯混合模型在聚类协方差方面比K-Means要灵活得多;根据标准差参数，聚类可以采用任何椭圆形状，而不是局限于圆形。其次，根据高斯混合模型的使用概率，每个数据点可以有多个聚类。因此，如果一个数据点位于两个重叠的聚类的中间，通过说X%属于1类，而y%属于2类，我们可以简单地定义它的类。



#### 评估

通过 NMI 对聚类结果进行评价.

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20150317154915532)



其中I(A,B)是A,B两向量的mutual information, H(A)是A向量的信息熵。 



### 实验结果

---------- KMeans ----------

KMeans cost time 38.968318462371826s, score is 0.781562372454415 

---------- AffinityPropagation ----------

AffinityPropagation cost time 13.451494693756104s, score is 0.7836988975391974 

---------- MeanShift ----------

MeanShift cost time 19.27985906600952s, score is 0.7278545708737196 

---------- SpectralClustering ----------

SpectralClustering cost time 40.73340439796448s, score is 0.8067027949127966 

---------- Ward Hierarchical Clustering ----------

Ward Hierarchical Clustering cost time 7.6284990310668945s, score is 0.7847178748775534 

---------- AgglomerativeClustering ----------

AgglomerativeClustering cost time 7.590268611907959s, score is 0.7847178748775534 

---------- DBSCAN ----------

DBSCAN cost time 46.81977677345276s, score is 0.611995415736244 

---------- GaussianMixture ----------

GaussianMixture cost time 5.616118431091309s, score is 0.7834852894647376 



可以看到虽然 SpectralClustering 方法效果好,但是时间花费的也很高.综合下来 GaussianMixture 应该是效果最好的.